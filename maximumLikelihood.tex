% BEGINING of PREAMBLE
\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[10pt]{article}

% FONTS
\usepackage{tgschola}
\usepackage[T1]{fontenc}

% SPACING and TEXT
\usepackage{microtype}
\usepackage[parfill]{parskip}
\usepackage{afterpage}

% GEOMETRY
\usepackage[paper  = letterpaper,
			left   = 1.5in,
			right  = 1.5in,
			top    = 1.0in,
            bottom = 1.0in,
            ]{geometry}

% COLOR
\usepackage[usenames,dvipsnames]{xcolor}

% FIGURES
\usepackage{graphicx}
\usepackage[labelfont=bf]{caption}
\usepackage[format=hang]{subcaption}

% TABLES
\usepackage{booktabs}

% MATH
\usepackage{amsmath, amsthm, amssymb}

% HYPERREF
\usepackage[colorlinks,linktoc=all]{hyperref}
\usepackage[all]{hypcap}
\hypersetup{citecolor = BurntOrange}
\hypersetup{linkcolor = MidnightBlue}
\hypersetup{urlcolor  = BrickRed}

% CLEVEREF must come after HYPERREF
\usepackage{cleveref}

\renewcommand{\theequation}{\thesection.\arabic{equation}}

% END of PREAMBLE

\title{{A Gentle Introduction to Maximum Likelihood for Undergraduate Engineering Audiences}}
\author{Alp Kucukelbir}
\date{\texttt{version 0.5 compiled on (\today)}}

\begin{document}
\maketitle

\section{Introduction}
You step on a scale to measure your weight, yet you are unsure of your scale's accuracy. So you take a second measurement of your weight. What do you do? You take the average of those two numbers. My question to you is, \emph{why?} Why not take the median? Or the mode? Or simply \emph{discard} the second number altogether?

Estimation theory is a beautiful and deep topic. Many  engineering and computer science courses provide an elementary appreciation for it, generally by working through some examples. Yet many students struggle with the concept (prior exposure to statistics/probability) and mechanics of the examples (jargon, set-up, and computation). My experience has been that these challenges confound the true insights of estimation theory.

My goal in this gentle introduction is to work through, in eye-watering detail, every step of a simple maximum likelihood estimation problem. As a nod to my electrical engineering background, I will ground the entire discussion in a seemingly trivial example of measuring voltage with a voltmeter. Despite its simplicity, this example will motivate many aspects of estimation theory and (hopefully!) answer that first question of \emph{why the mean?}

\section{Probabilistic Modeling}
The world is not deterministic. There is uncertainty (noise) in our measurements. Probability theory is a powerful framework in which to study and model uncertainty. This is why we use probability to setup estimation problems.

So let's start with our basic example. We have a circuit board and a voltmeter. Our goal is to use our voltmeter to measure the voltage at a certain point on the circuit board. In the absence of any uncertainty (noise), this is what the measurement would look like
\begin{align}
	x = v,\label{relationshipModel}
\end{align}
where $x$ is our measurement (the thing we read off the voltmeter) and $v$ is the \emph{true} voltage at that point in the terminal. This may seem like a pointless equation, but it is critical to understand that \emph{philosophically}, what you measure is \textbf{not} the true thing. The measurement $x$ is the result of an action: in this case, plugging a voltmeter into the board and reading off the number. The true thing is the voltage $v$ that actually exists on the board.

The goal of probabilistic modeling is two-fold: 1) to \emph{explain} the relationship between your measurement and the true thing, and 2) to \emph{describe} the uncertainty in your measurement. In our case, point 1 is trivial: we measure the voltage directly as explained by \Cref{relationshipModel}. Since we've covered point 1) above, let's move on to point 2).

I now tell you that the voltmeter is not perfect. I have determined that in reality it tends to introduce small errors in my measurements. I have realized that these errors seem to be more or less in the $[-5, 5]$ volt range while an overwhelming amount of the errors are actually between $[-3, 3]$. Therefore, I \textbf{decide} to \textbf{model} this ``real-world'' phenomenon as follows,
\begin{align}
	x &= v + \eta, \label{measurementModel}\\
	\eta &\sim \mathcal{N}(0,1)\label{noiseModel}.
\end{align}

Now, what does this say? Remember, I always encourage you to read equations out aloud in plain English. So let's do that.

\Cref{measurementModel} says that I measure some number $x$ from my voltmeter. This number is equal to the true thing (voltage $v$) plus some other number $\eta$.

\Cref{noiseModel} says that $\eta$ is random. This means that if I take one measurement, I will get $x_1 = v + \eta_1$. If I then take another measurement, I will get $x_2 = v + \eta_2$. Note that the true voltage $v$ is the same, while the thing that makes $x_1 \neq x_2$ is that $\eta_1 \neq \eta_2$.

Now, \cref{noiseModel} says something more about $\eta$. Not only is $\eta$ random, but we also know how it is distributed. In this case, it is distributed as $\mathcal{N}(0,1)$, a Gaussian centered around $0$ with standard deviation $1$. This is my model for how my voltmeter makes mistakes in its measurements.

Note that I have not said anything about estimation yet. I am simply \emph{describing} the uncertainty in my measurements. I am \textbf{probabilistically modeling} the physics of my voltmeter. That is all. This may be a bad model of reality, in which case I will likely do a poor job of estimating the voltage. Many real-world problems were difficult to tackle until someone came along and proposed a good probabilistic model. Let's assume, for now, that this is a decent model.

In most undergraduate engineering classes, this information will \emph{almost} always be given to you. Your job is to simply understand what is going on and make sure you grasp the subtleties between the true thing and measurements of that true thing. Probability comes in as a tool to describe the uncertainty in your measurements.

Now that we've established the model, we can talk about probabilities of measurements.

\section{Probabilities of Measurements}
First, let's make our lives a bit easier by combining Equations \eqref{measurementModel} and \eqref{noiseModel} into
\begin{align}
	x &\sim \mathcal{N}(v,1),
\end{align}
which says: my measurement $x$ is a random number that follows a Gaussian distribution with mean equal to the true voltage $v$ and standard deviation of $1$ volt.

Another way of saying this, is by explicitly writing out the \emph{probability} of measuring some value of $x$. Naturally, you should expect this to be some function of the true voltage $v$, and for it to take large values around $v$ and smaller values away from $v$. To state this rigorously, I need to \emph{fix} the value of the voltage $v$ to some value, say $v = v_f$. With this, the probability of measuring some value of $x$ given $v$ is
\begin{align}
	p(x \vert v=v_f) &= \frac{1}{\sqrt{2\pi}}\,\exp\left( -\frac{(x-v_f)^2}{2} \right) \label{prob}.
\end{align}
Again, read this out in plain English. This says that, if the voltage on the circuit board happened to be some fixed voltage $v_f$, then the probability of measuring some number on my voltmeter is given by the right hand side of \Cref{prob}. So let's plug some numbers in, just for fun. Let's say $v_f = 5$ volts. What is the probability of observing $x=4.7$?
\begin{align}
	p(x=4.7 \vert v = 5) &= \frac{1}{\sqrt{2\pi}}\,\exp\left(-\frac{(4.7-5)^2}{2} \right)\\
						 &= 0.3814
\end{align}
In comparison, what is the probability of observing $x=2$?
\begin{align}
	p(x=2 \vert v = 5) &= \frac{1}{\sqrt{2\pi}}\,\exp\left(-\frac{(2-5)^2}{2} \right)\\
						 &= 0.0044
\end{align}
So, we are more likely to measure 4.7 volts rather than 2 volts, given that the true voltage is 5 volts. This seems to match our understanding of reality and how well our voltmeter works. Great.

In practice, we tend to drop the explicit notation of $v=v_f$. The following equation is completely equivalent to \cref{prob}
\begin{align}
	p(x \vert v) &= \frac{1}{\sqrt{2\pi}}\,\exp\left( -\frac{(x-v)^2}{2} \right).
\end{align}
If you get confused, simply re-introduce the explicit notation during your calculations. Do not worry: you will quickly become accustomed to the implicit notation and save the planet with your reduced ink consumption.

Up to this point, we've been talking about \emph{single} measurements. Now we are going to switch to talking about \emph{multiple} measurements. The reason is, just like the weighing example in the introduction, if you don't trust your voltmeter, you will want to take multiple measurements. Before we address \emph{what to do} with those measurements, we need to introduce the concept of \emph{likelihood functions} and their subtleties.

\section{Likelihood}

Let's say that we measure the voltage $n$ times, $x_1, x_2, \ldots, x_n$, and stack all those measurements into a vector $X = \{x_1, x_2, \ldots, x_n\}$. If we know that these measurements were done independently, we can compute the joint probability as a product of the single measurement probabilities. Therefore, the joint probability of observing all $n$ measurements given the true voltage $v$ (which doesn't change between measurements) is
\begin{align}
	p(X\vert v) &= \prod_{i=1}^n p(x_i|v)\\
				&= \prod_{i=1}^n \frac{1}{\sqrt{2\pi}}\,\exp\left( -\frac{(x_i-v)^2}{2} \right).\label{likelihood}
\end{align}
This is the \emph{likelihood function}
\begin{align}
	l(X \vert \theta) &= p(X \vert v),\\
	\theta &= v.
\end{align}
In plain English, it says: the probability of measuring $n$ independent numbers from my voltmeter is equal to the product of measuring each of those numbers separately. Nothing outrageous.

Now in \emph{likelihood} parlance, we like to talk about the likelihood being parametrized by $\theta$. This is simply another way of saying that the likelihood distribution depends on the value of $\theta$. In our case, the measurements $X$ will clearly depend on what the true value of the voltage $v$ is. Therefore, our parameter $\theta$ is $v$, and $X$ is the data (our measurements).

We've spent three full pages talking about all sorts of interesting things, but now we finally get to the goal of this endeavor: estimating $\theta$ from $X$. Or in plain English, what do you do with $n$ measurements from your voltmeter if what you want is the true voltage $v$ in your circuit.

\section{Maximum Likelihood Estimation}
Maximum Likelihood (ML) estimation is a \emph{method} of estimating the parameter (what you want) from your data (what you measure). Stated simply, it says this: to estimate $\theta$, first probabilistically model your problem (Sections 2 \& 3), then calculate the likelihood of your measurements (Section 4), and finally find the $\theta$ that maximizes your likelihood. Mathematically, this looks like
\begin{align}
 	\widehat{\theta}_{ML} = \arg \max_\theta\, l(X\vert\theta),
 \end{align}
where $\widehat{\theta}_{ML}$ is the maximum likelihood \textbf{estimate} of $\theta$, and $ \arg \max_\theta$ means `find the $\theta$ that maximizes the following function.' The right hand side of this equation will eventually be a function of the data $X$. This function is called the maximum likelihood \textbf{estimator}. The estimate is the output of the estimator; do not get these two terms mixed up.

Let's first understand the insight behind this. Remember what the likelihood is. The likelihood is the probability of observing $n$ measurements as a function of the parameter $\theta$. In our case, it is the probability of measuring $n$ values from our voltmeter, given that the true voltage is $v$. What maximum likelihood tells you is, to estimate the true voltage, just find the value of $v$ that maximizes the probability of observing the measurements. This is the ``most likely'' value of $v$.

So, I claim that, for our small example here, the maximum likelihood estimator is
\begin{align}
	\widehat{v}_{ML} &= \overline{X}\\
					 &= \frac{1}{n}\sum_{i=1}^n x_i.
\end{align}
You should verify this (\emph{do it, please!}) by checking if this is indeed the maximizer of \Cref{likelihood}. A useful trick is to take the logarithm of the likelihood, because of this property
\begin{align}
	\arg \max_\theta\, l(X\vert\theta) = \arg \max_\theta\, \log\left(l(X\vert\theta)\right).
\end{align}
This always holds as the logarithm is a monotonically increasing function. (It does not modify the maxima/minima of the function.)

So now we have answered the question of why we should take the mean of our voltmeter measurements! Because taking the mean is the maximum likelihood estimator of the true voltage. And so it enjoys all the wonderful properties of maximum likelihood estimation, which I presume you will cover in class.

(\emph{I will probably include a brief discussion to wrap up this section in the future.})

(\emph{I will also probably expand this simple example to MAP estimation.})

\section{Version History}
\begin{description}
	\item[\texttt{version 0.5:}] Style changes and minor clarifications.
	\item[\texttt{version 0.4:}] Style changes and some typos.
	\item[\texttt{version 0.3:}] Typos, thanks to comments from an anonymous EENG 202 alum.
	\item[\texttt{version 0.2:}] Expanded for general audiences.
	\item[\texttt{version 0.1:}] Initial draft, written as a supplement for Yale University's EENG 202 class.
\end{description}

\section{Colophon}
A Gentle Introduction to Maximum Likelihood for Undergraduate Engineering Students by Alp Kucukelbir is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License.

The \LaTeX{} source and license lives at
\begin{center}
	\url{https://github.com/akucukelbir/maximumLikelihood}
\end{center}
\end{document}
























